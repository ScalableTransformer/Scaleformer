\documentclass[12pt]{article}
\usepackage{paper-scaleformer}
\usepackage{float}
\usepackage{multicol}
\usepackage{caption}
\usepackage{esvect}
\usepackage{xfrac}
\usepackage{geometry}
\geometry{margin=20mm}


\newlength\mylen
\newcommand\NextInput[1]{%
	\settowidth\mylen{\KwIn{}}%
	\setlength\hangindent{\mylen}%
	\hspace*{\mylen}#1\\}

\newcommand\NextData[1]{%
	\settowidth\mylen{\KwData{}}%
	\setlength\hangindent{\mylen}%
	\hspace*{\mylen}#1\\}

%\twocolumn

\begin{document}
\maketitle%

\begin{abstract}
To overcome the quadratic complexity with sequence length of the original transformer, some previous works proposed a kernelized attention mechanism which can scale linearly depending on operation orders. Other works proposed to change the way the position of each token is encoded so that the model depends on relative distance between tokens instead of absolute position. In this work we propose a novel algorithm to combine kernelized attention with relative positional encoding while still scaling linearly in complexity.
\end{abstract}

\clearpage
\begin{multicols}{2}

\input{sections/01-introduction.tex}
\input{sections/A_rel_S_rel_naive_calculation.tex}
\input{sections/02-background.tex}
\input{sections/A_rel_linear_calculation.tex}
\input{sections/03-masked-attention.tex}
\input{sections/A_masked_algorithm.tex}
\input{sections/04-rpe-implementation.tex}
\input{sections/A_rel_algorithm.tex}
\input{sections/timings.tex}
\input{sections/05-scalable-transformer.tex}
\input{sections/06-results.tex}
\input{sections/07-conclusions.tex}

\end{multicols}{}

\input{sections/table-of-symbols.tex}

\clearpage

\bibliography{paper-scaleformer}

\end{document}
