\documentclass[12pt]{article}
\usepackage{paper-scaleformer}

\begin{document}
\maketitle%

\begin{abstract}
To overcome the quadratic complexity of the vanilla transformer with sequence length, some previous works proposed a kernelized attention mechanism, which has a linear complexity. Other works proposed the change the way the position of each token is encoded so that the model depends on relative distance between tokens instead of absolute position. In this work we propose a model that combines kernelized attention with relative positional encoding while still scaling linearly in complexity. Furthermore we propose to dynamically chose the most favourable order of commutative operations. While giving the same results, different operation orders have memory usage and computational cost that depends differently on embedding dimension and sequence length.
\end{abstract}

\input{sections/01-introduction.tex}
\input{sections/02-background.tex}
\input{sections/03-masked-attention.tex}
\input{sections/04-rpe-implementation.tex}
\input{sections/05-scalable-transformer.tex}
\input{sections/06-results.tex}
\input{sections/07-conclusions.tex}
\clearpage

\bibliography{paper-scaleformer}

\end{document}
