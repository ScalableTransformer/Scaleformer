\documentclass[12pt]{article}
\usepackage{paper-scaleformer}

\begin{document}
\maketitle%

\begin{abstract}
To overcome the quadratic complexity with sequence length of the vanilla transformer, some previous works proposed a kernelized attention mechanism which scales linearly. Other works proposed to change the way the position of each token is encoded so that the model depends on relative distance between tokens instead of absolute position. In this work we propose to combine kernelized attention with relative positional encoding while still scaling linearly in complexity thanks to a novel algorithm.
\end{abstract}

\input{sections/01-introduction.tex}
\input{sections/02-background.tex}
\input{sections/03-masked-attention.tex}
\input{sections/04-rpe-implementation.tex}
\input{sections/05-scalable-transformer.tex}
\input{sections/06-results.tex}
\clearpage
\input{sections/07-conclusions.tex}
\input{sections/table-of-symbols.tex}
\clearpage
\bibliography{paper-scaleformer}

\end{document}
