\section{Conclusion}In the present work an algorithm to compute\citet{shaw2018selfattention} relativepositional encoding with linear complexity has been presented. An implementation of \citet{choromanski2021rethinking} prefix sum algorithm that does not requires custom CUDA code - while maintaining linear complexity - was presented as well. These two elements allowed to define a kernelized attention function with relative positional encoding that can be computed with linear complexity with regards to sequence length. This opens the path to sequence to sequence models trained on bigger document sizes, that better generalize to sequences longer than the ones used during training thanks to relative positional encoding.Several tasks would benefit from being applied on longer sequences. Chat bots trained on full conversations could take into account informations delivered several exchanges before. Machine translation models applied on whole documents would have more information of the context than simple sentence to sentence translations.\endinput