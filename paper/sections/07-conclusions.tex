\section{Conclusion}In the present work an algorithm to compute\citet{shaw2018selfattention} relativepositional embedding with linear complexity has been presented. An implementation of \citet{choromanski2021rethinking} prefix sum algorithm that doesn't requires custom CUDA code - while maintaining linear complexity - was also presented. These two elements allowed to define a kernelized attention function with relative positional encoding, that can be computed with linear complexity with regards to sequence length. This opens the path to sequence to sequence models that can be applied to massive document sizes. For example chat bots trained on small sequence length, that can generalize to long conversations thanks to relative positional encoding, and maintain fast evaluation times thanks to linear complexity.The linear complexity algorithm is a trade-of of quadratic complexity with sequence length for quadratic complexity with embedding dimension. Increasing expressiveness power of the model can however be done efficiently by increasing the number of heads. As both the naive and linear algorithm have linear complexity with the number of attention heads.\endinput