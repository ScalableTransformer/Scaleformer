\section{Conclusion}In the present work an algorithm to compute\citet{shaw2018selfattention} relativepositional embedding with linear complexity has been presented. An implementation of \citet{choromanski2021rethinking} prefix sum algorithm that doesn't requires custom CUDA code - while maintaining linear complexity - was also presented. These two elements allowed to define a kernelized attention function with relative positional encoding, that can be computed with linear complexity with regards to sequence length. This opens the path to sequence to sequence models trained on bigger document size, that better generalize to sequences longer than during training thanks to relative positional encoding.Several tasks would benefit from being applied on longer sequences. Chat bots trained on full conversations could take into account informations delivered several exchanges before. Machine translation models applied on whole documents would have more information of the context than simple sentence to sentence translations.\endinput