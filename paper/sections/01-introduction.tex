\section{\label{sec:introduction}Introduction}\citet{vaswani2017attention} introduced the transformers, a novel architecture to model sequence-to-sequence translations tackling new heights of problem complexity. The improvements were due to the non sequential nature of its training process which allowed better parallelization and thus fast training of big models, and due to improved long term dependencies thanks to the intrinsic good gradient flows of the attention mechanism. However the original transformer presents some drawbacks. It has a quadratic complexity with sequences length. The positional encoding is detrimental to the generalization of the meaning of a token at a new position, unseen during training.Ever since, significant research effort have been made to alleviate these problems. Several strategies have been proposed to reduce the quadratic complexity with sequence length, and the relative positional encoding further improved the performances of the model. These improvements have opened the path to the application of transformers to image analysis where scalability and positional invariance are of upmost importance. However most proposed architecture we are aware of only alleviated parts of the issues, by being incompatible with relative positional encoding, having no scheme for masked attention - thus being restrained to encoder-only models, or being complex to use/implement - needing custom operations programmed in CUDA or introducing stochastic methods.In the present work we propose a complete encoder-decoder transformer model that scales linearly with large sequence lengths, by putting together ideas developed across several works \cite{vaswani2017attention,shen2020efficient,katharopoulos2020transformers,choromanski2021rethinking,shaw2018selfattention,horn2021translational}. It remains compatible with relative positional encoding, and can be implemented with usual functions of neural network frameworks without requiring custom CUDA code.\endinput