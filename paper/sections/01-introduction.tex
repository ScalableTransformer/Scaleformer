\section{\label{sec:introduction}Introduction}\citet{vaswani2017attention} introduced the transformers, a novel architecture for sequence-to-sequence tasks, tackling new heights of problem complexity. The improvements were due to the non sequential nature of its training process which allowed better parallelization and thus fast training of big models, and due to improved long term dependencies thanks to the intrinsic good gradient flows of the attention mechanism. However the original transformer presents some drawbacks. It has a quadratic complexity with sequences length. And its absolute position encoding is detrimental to its extrapolation to new sequence lengths.Ever since, significant efforts have been made to alleviate these problems. These improvements have opened the path to the application of transformers to image analysis where scalability and positional invariance are essential. However most proposed architecture we are aware of only alleviated parts of the issues, by being incompatible with relative positional encoding, having no scheme for masked attention - thus being restrained to encoder-only models, or being complex to use/implement - needing custom operations programmed in CUDA or introducing stochastic methods.In the present work we propose a complete encoder-decoder transformer model that scales linearly with sequence lengths, by putting together ideas developed across several works \cite{vaswani2017attention,shen2020efficient,katharopoulos2020transformers,choromanski2021rethinking,shaw2018selfattention,horn2021translational}. It remains compatible with relative positional encoding, and can be implemented with usual functions of neural network frameworks without requiring custom CUDA code.\endinput