\section{The Scaleformer}The proposed model replaces the scaled-dot-product-attention by akernelized attention with RPE. Following the observations of\citet{shaw2018selfattention} thataccumulating absolute positional encoding with relative positionalencoding yield no benefits, the positional encoding is also removed.In this work we have chosen the following formulation, with $\phi(x) = exp(x)$.\begin{equation}A = \frac{\left( \phi(Q) \times \phi(K^T) + S^{rel} \right)}{\sum_j \left( \phi(Q) \times \phi(K^T) + S^{rel} \right)} \times V\end{equation}This is essentially a combination of two terms: the kernelized attentionproposed by  \citet{katharopoulos2020transformers}, and the relative positional encoding proposed by\citet{shaw2018selfattention}. The leftterm is the score matrix of shape $(L_Q, L_K)$, with a denominatorwhich scales all rows so that they sum to 1.For the linear complexity implementation, the multiplication must be distributed as:\begin{equation}A = \frac{\left( \phi(Q) \times \phi(K^T) \times V \right) + \left( S^{rel} \times V\right)}{\sum_j \left( \phi(Q) \times \phi(K^T) \right) + \sum_j \left( S^{rel} \right)}\end{equation}The denominator can be easily calculated by applying the linear complexity algorithms with V replaced by a matrix of shape $(L_K, 1)$ full of 1, or by summing the rows of the score matrix for quadratic complexity algorithm.\endinput