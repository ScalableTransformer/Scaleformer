\section{Background}

\subsection{The original multi-head attentionmechanism}

The scaled dot-product attention proposed by\citet{vaswani2017attention} transforms the vectorial embedding $\vec{Y}_i$ of a token as a function of a sequence of other embedding $\vec{X}_j$. Where $\vec{Y}_i$ and $\vec{X}_j$ are all vectors of size $D$. A key$\vec{K}_j$ and value $\vec{V}_j$ are attributed to each vector$\vec{X}_j$, and query $\vec{Q}_i$ is attributed to $\vec{Y}_i$. The vectors are obtained by linear projection from dimension $D$ to $d$ using three matrices of learnable parameters. The new vector $\vec{y}_i'$ is a weighted sum of the $\vec{V}_j$. The weights are scores of matching between the query $\vec{Q}_i$ and the keys$\vec{K}_j$, calculated as the dot product between the two vectors.They are also \emph{softmaxed} to sum up to 1. The transformation of $L_Q$vectors $\vec{Y}_i$ as a function of $L_K$ vectors $\vec{X}_j$ can be efficiently computed with matrix multiplications:

\begin{equation}
A = \mathrm{softmax}\left(\frac{Q \times K^T}{\sqrt{d}}\right) \times V
\end{equation}

With $Q$ a matrix of shape $(L_Q, d)$, $K$ a matrix of shape $(L_K, d)$ and $V$ a matrix of shape $(L_K, d)$. The $\sqrt{d}$at the denominator is a scaling factor used to avoid saturation in theexponential terms of the softmax function.

The multi-head attention performs $h$ different projections into spaces of dimension $d = D/h$. The resulting vector $\vec{Y}_i'$ is theconcatenation of the $h$ vectors $\vec{y}_i'$ obtained. Thus theembedding dimension is preserved. Using multiple heads was foundbeneficial by the authors over using a single head of dimension$d = D$.

During training, the cross entropy of the $n^{th}$ predicted token is calculated assuming all previous tokens have been generated correctly. This enables to parallelize training completely without need forrecurrence. However as the $n^{th}$ token should not depend of thefollowing tokens, the cells in the upper right corner of the scorematrix are set to $-\infty$ such that after the softmax they are equalto 0, and the rows still sums up to 1.

\subsection{Improving Transformer scalability with sequencelength}

The original attention mechanism requires the computation of a scorematrix $Q \times K^T$ of shape $(L_Q, L_K)$, with complexity$O(L_QdL_K)$. If the query and key sequence lengths are multiplied bytwo, then the memory used and computation time are multiplied by 4. Toimprove the scalability of the transformer with sequence length, several axis of research have been explored.

\citet{kitaev2020reformer} proposedthe Reformer's architecture, which uses an hash-bucketting algorithm toreduce the complexity of the original multi head attention operationfrom $O(L^2)$ to $O(L\log(L))$.

\citet{dai2019transformerxl} proposed theTransformer-XL's architecture, which cuts the sequence in segments oflength L. The model predicts each stage of the current segment as afunction of the previous and current segment. All the segments arecomputed sequentially with a recurrence mechanism. The complexity is linearwith sequence length, but the computation cannot be completely parallelized due to the recurrence mechanism, although more than a RNN,as segments can be computed in one go.

Other publications explored using a sparse attention matrix, such as the Longformer by \citet{beltagy2020longformer} and the Big Bird model by \citet{zaheer2021big}. As eachtoken attends to a fixed number of all other tokens, the scalability isimproved. These sparse attention models however require customoperations implemented in CUDA.

Some other works propose to modify the attention mechanism to be compatible with linear complexity. The Linformer by\citet{wang2020linformer} projects the key and values onto a smaller sequence length dimension with matrix multiplication. It cannot however generalize to sequences longer than during training, as the weights of the projection for such tokens would be undefined.

\citet{shen2020efficient} proposed to replace the softmax attention score. $A = \mathrm{softmax}\left(\frac{Q \times K^T}{\sqrt{d}}\right) \times V$ ischanged into $A = \rho(Q) \times \rho(K)^T \times V$. With $\rho$the softmax function along the embedding dimension. Thanks to matrixmultiplication commutativity, the order of the operations can be chosen.If $Q$, $K$ and $V$ are of shape $(L_Q, d)$, $(L_K, d)$ and$(L_K, d)$ respectively, the complexity of$(\phi(Q) \times \phi(K)^T) \times V$ is $O(L_Q \times d \times L_K)$ whereas the complexity of$\phi(Q) \times (\phi(K)^T \times V)$ is$O\left(max(L_Q, L_k) \times d^2 \right)$. The right-side-firstoperation is linear in complexity with sequence length. The shape of theintermediate result matrix is also changed, allowing to scale the betterin memory requirements as well. The original \emph{softmaxed} attention scorematrix was giving rows of positive scores that sum to 1. With thischange the elements of the score matrix remain positive as $\phi(Q)$and $\phi(K)^T$ are matrices of positive values, but the rows of thescore matrix does not sum up to 1. This work also does not give a linear complexity formulation for masked attention. If the right-side-firstscheme is adopted, the attention score matrix$\phi(Q) \times \phi(K)^T$ is never explicitly computed, and can't bemasked.

Building on this idea of commutative attention function proposed by
\citep{shen2020efficient}, \citet{katharopoulos2020transformers} introduced their kernerlized attention function as:

\begin{equation}
A = \frac{\phi(Q) \times \phi(K)^T}{\sum_j \left( \phi(Q) \times \phi(K)^T \right)} \times V
\end{equation}

The function $\phi$ is applied element-wise and can be any positivefunction, for example $\phi(x) = elu(x) + 1$. This attention isrow-wise normalized so that all rows of the score matrix are sets ofpositive weights adding up to one. This preserves the objective of theoriginal softmaxed attention scores, while allowing to performoperations in an optimal order.

The Performer by \citet{choromanski2021rethinking} exploits the same idea of a kernelized attention introducedby \citet{katharopoulos2020transformers}, with an algorithm that betters approximate softmaxed attention. Mostimportantly they also give in annex a prefix sum algorithm to performoperations in the right-side-first order while giving the same result asmasked left-side-first operation.

Although the author did not specify how to implement it, the only implementations we found of this operation requires custom CUDA code. In this work we will give an implementation of the right-side-first maskedoperation, with usual functions from neural network frameworks, thatremains linear in complexity.

\subsection{Alternatives to absolute positionalencoding}

The original multi-head attention operation introduced by\citet{vaswani2017attention} wasintrinsically invariant by token order permutation. As token position wasan important information for sequence to sequence models, they encodedthe global position of each token in their embedding. Since then, somemodified attention mechanisms, that depend on relative tokens position,have been proposed.

\citet{shaw2018selfattention} exploredmodifying the attention mechanism so that it depends on the relativedistance between tokens. A second score matrix that is function of thequery and the query/key relative distance is added to the original scorematrix that only depends on query/key vector representation. $A = softmax\left(\frac{Q \times K^T}{\sqrt{d}}\right) \times V$ becomes$A = \left(\frac{Q \times K^T + S_{rel}}{\sqrt{d}}\right) \times V$ with $S_{rel}$ of shape $(L_Q, L_K)$ defined as${S_{rel}}_{ij} = \vec{Q_i} \cdotp \vec{RP}_{clip(i-j, -k, k)}$. Where $k$ is the attention horizon length and $\vec{RP}_n$ is one of$2k+1$ relative positional embedding, vectors of size $d$.\citet{shaw2018selfattention} and\citet{huang2018music} observed that introducing this attention scheme improved performances.The naive calculation of this term however has a complexity of$O(L_QL_Kd)$. No algorithm was provided to linearize the complexity.

More recently \citet{liutkus2021relative} gives a stochastic positional encoding that is linear incomplexity with regards to sequence length. However the implementationis complex and its stochastic nature requires that the operations berepeated several times in parallel.

\citet{horn2021translational} noted thatthe term $S^{rel} \times V$ can be computed with linear complexity forthe case where $RP_{-k} = RP_{k}$. However this is restraining as themodel can't make the difference between tokens before the attentionhorizon or after.

In this work we will show that the computation of $S^{rel} \times V$can also be done with linear complexity, without concession.

\endinput
