\section{Linear complexity masked attention}

In this section we will detail the prefix sum algorithm proposed by
\citet{choromanski2021rethinking} for
 masked kernelized attention, and give an implementation with usual
functions of neural network frameworks. The masked kernelized attention is defined as:

\begin{equation}
A^{masked} = \mathrm{masked} \left( \phi(Q) \times \phi(K)^T \right) \times V
\end{equation}

In this expression, $\mathrm{masked}$ being the operation that set all cells
 above the diagonal to 0 in a matrix. The naive implementation of this
 operation has complexity $O(L_QL_Kd)$.

We can change the complexity using the operation proposed by
 \citet{choromanski2021rethinking}. We
 will derive its formulation here.
 To start with it, $A^{masked}$ is expressed as

\begin{equation}
A^{masked} = S^{masked} \times V
\end{equation}

\noindent{}which
in summation form is expressed as

\begin{equation}
A^{masked}_{ij} = \sum_k V_{kj} \times S^{masked}_{ik}
\end{equation}

\noindent{}and the
elements of S are defined as:

\begin{equation}
S^{masked}_{ik} =
\begin{cases}
k \leq i & \sum_l \left( \phi(Q)_{il} \times \phi(K)_{kl} \right) \\
\text{otherwise} &{}0 
\end{cases}
\end{equation}

Putting these elements together leads to

\begin{equation}
A^{masked}_{ij}= \sum_{k=1}^i V_{kj} \times \sum_l \left( \phi(Q)_{il} \times \phi(K)_{kl} \right)
\end{equation}

\noindent{}which can be reworked as

\begin{equation}
A^{masked}_{ij}= \sum_l \phi(Q)_{il} \times \sum_{k=1}^i \left(V_{kj} \times \phi(K)_{kl} \right)
\end{equation}

In this work we make use of these ideas to implement the calculation of
 $A^{masked}$ with complexity $O(max(L_Q, L_K) \times d^2)$ without
 custom GPU code as per the algorithm \ref{alg:A_masked}. With $align(tensor, n)$ the function that truncates or repeats the last value so that $tensor$ has length $n$ along the first dimension, $zeros(a, b, ...)$ the 0-initialized tensor of shape $(a, b, ...)$, and $cumsum(tensor)$ the function that returns the cumulated sum along the first dimension.

\begin{algorithm}[H]
	\caption{calculation of $A^{masked}$ with linear complexity}
	\label{alg:A_masked}
	\KwData{$\phi(Q)$, $\phi(K)$, $V$ tensors of shape $(L_Q, d)$, $(L_K, d)$, and $(L_K, d)$}
	\KwResult{$A^{masked}$ tensor of shape $(L_Q, d)$}
	$\text{Unrolled} := zeros(L_K, d, d)$\\
	\For{$i=0$ \KwTo $L_Q-1$}
	{
		\For{$j=0$ \KwTo $d-1$}
		{
			$\text{Unrolled}_{kjl} = V_{kj} \times \phi(K)_{kl}$
		}
	}
	$\text{Right} := cumsum\left(\text{Unrolled}, \text{dim}=0\right)$\\
	$\text{Aligned} := align(\text{Right}, L_Q)$\\
	$A^{masked} := zeros(L_Q, d)$\\
	\For{$i=0$ \KwTo $L_Q-1$}
	{
		\For{$j=0$ \KwTo $d-1$}
		{
			$A^{masked}_{ij} = \sum_k \left( \phi(Q)_{ik} \times \text{Aligned}_{ikj} \right)$
		}
	}
\end{algorithm}
