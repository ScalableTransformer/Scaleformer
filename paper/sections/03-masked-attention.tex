\section{Masked attention
implementation}

In this section we will detail the prefix sum algorithm proposed by
\citet{choromanski2021rethinking} for
 masked kernelized attention, and give an implementation with usual
functions of neural network frameworks.

\begin{equation}
A^{masked} = \mathrm{masked} \left( \phi(Q) \times \phi(K^T) \right) \times V
\end{equation}

In this expression, $\mathrm{masked}$ being the operation that set all cells
 above the diagonal to 0 in a matrix. The naive implementation of this
 operation has complexity $O(L_QL_Kd)$.

We can change the complexity using the operation proposed by
 \citet{choromanski2021rethinking}. We
 will derive its formulation here.
 To start with it, $A^{masked}$ is defined as

\begin{equation}
A^{masked} = S^{masked} \times V
\end{equation}

\noindent{}which
in summation form is expressed as

\begin{equation}
A^{masked}_{ij} = \sum_k V_{kj} \times S^{masked}_{ik}
\end{equation}

\noindent{}and the
elements of S are defined as:

\begin{equation}
S^{masked}_{ik} =
\begin{cases}
k \leq i & \sum_l \left( \phi(Q)_{il} \times \phi(K)_{kl} \right) \\
\text{otherwise} &{}0 
\end{cases}
\end{equation}

Putting these elements together leads to

\begin{equation}
A^{masked}_{ij}= \sum_{k=1}^i V_{kj} \times \sum_l \left( \phi(Q)_{il} \times \phi(K)_{kl} \right)
\end{equation}

\noindent{}which can be reworked as

\begin{equation}
A^{masked}_{ij}= \sum_l \phi(Q)_{il} \times \sum_{k=1}^i \left(V_{kj} \times \phi(K)_{kl} \right)
\end{equation}

In this work we make use of these ideas to implement the calculation of
 $A^{masked}$ with complexity $O(max(L_Q, L_K) \times d^2)$ without
 custom GPU code as per the algorithm \ref{alg:A_masked}.

\begin{algorithm}[H]
	\caption{calculation of $A^{masked}$ with linear complexity}
	\label{alg:A_masked}
	\KwData{$\phi(Q)$, $\phi(K)$, $V$ tensors of shape $(L_Q, d)$, $(L_K, d)$, and $(L_K, d)$}
	\KwResult{$A^{masked}$ tensor of shape $(L_Q, d)$}
	$\text{UNROLLED}_{kjl} := V_{kj} \times \phi(K)_{kl}$ tensor of shape $(L_K, d, d)$\\
	$\text{RIGHT} := cumsum\left(\text{UNROLLED}, \text{dim}=0\right)$\\
	\uIf{$L_{K} < L_{Q}$}
	{
		$\text{LAST}_{i,j,k} := RIGHT_{L_K-1,j,k}$ tensor of shape $(L_Q - L_K, d, d)$\\
		$\text{RIGHT} := concatenate\left(\text{RIGHT}, \text{LAST}, \text{dim}=0\right)$
	}
	\ElseIf{$L_{K} > L_{Q}$}
	{
		$\text{RIGHT} := \text{RIGHT}_{[:L_Q, :, :]}$
	}
	$A^{masked}_{ij} = \sum_k \left( \phi(Q)_{ik} \times \text{RIGHT}_{ikj} \right)$
\end{algorithm}
