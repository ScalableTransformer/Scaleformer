% Encoding: UTF-8

@Misc{vaswani2017attention,
  author        = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  title         = {Attention Is All You Need},
  year          = {2017},
  archiveprefix = {arXiv},
  eprint        = {1706.03762},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/1706.03762},
}

@Misc{shen2020efficient,
  author        = {Zhuoran Shen and Mingyuan Zhang and Haiyu Zhao and Shuai Yi and Hongsheng Li},
  title         = {Efficient Attention: Attention with Linear Complexities},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {1812.01243},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1812.01243},
}

@Misc{katharopoulos2020transformers,
  author        = {Angelos Katharopoulos and Apoorv Vyas and Nikolaos Pappas and François Fleuret},
  title         = {Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {2006.16236},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2006.16236},
}

@Misc{choromanski2021rethinking,
  author        = {Krzysztof Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tamas Sarlos and Peter Hawkins and Jared Davis and Afroz Mohiuddin and Lukasz Kaiser and David Belanger and Lucy Colwell and Adrian Weller},
  title         = {Rethinking Attention with Performers},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2009.14794},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2009.14794},
}

@Misc{shaw2018selfattention,
  author        = {Peter Shaw and Jakob Uszkoreit and Ashish Vaswani},
  title         = {Self-Attention with Relative Position Representations},
  year          = {2018},
  archiveprefix = {arXiv},
  eprint        = {1803.02155},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/1803.02155},
}

@Misc{horn2021translational,
  author        = {Max Horn and Kumar Shridhar and Elrich Groenewald and Philipp F. M. Baumann},
  title         = {Translational Equivariance in Kernelizable Attention},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2102.07680},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2102.07680},
}

@Misc{kitaev2020reformer,
  author        = {Nikita Kitaev and Łukasz Kaiser and Anselm Levskaya},
  title         = {Reformer: The Efficient Transformer},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {2001.04451},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2001.04451},
}

@Misc{dai2019transformerxl,
  author        = {Zihang Dai and Zhilin Yang and Yiming Yang and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},
  title         = {Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context},
  year          = {2019},
  archiveprefix = {arXiv},
  eprint        = {1901.02860},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1901.02860},
}

@Misc{beltagy2020longformer,
  author        = {Iz Beltagy and Matthew E. Peters and Arman Cohan},
  title         = {Longformer: The Long-Document Transformer},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {2004.05150},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2004.05150},
}

@Misc{zaheer2021big,
  author        = {Manzil Zaheer and Guru Guruganesh and Avinava Dubey and Joshua Ainslie and Chris Alberti and Santiago Ontanon and Philip Pham and Anirudh Ravula and Qifan Wang and Li Yang and Amr Ahmed},
  title         = {Big Bird: Transformers for Longer Sequences},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2007.14062},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2007.14062},
}

@Misc{wang2020linformer,
  author        = {Sinong Wang and Belinda Z. Li and Madian Khabsa and Han Fang and Hao Ma},
  title         = {Linformer: Self-Attention with Linear Complexity},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {2006.04768},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2006.04768},
}

@Misc{huang2018music,
  author        = {Cheng-Zhi Anna Huang and Ashish Vaswani and Jakob Uszkoreit and Noam Shazeer and Ian Simon and Curtis Hawthorne and Andrew M. Dai and Matthew D. Hoffman and Monica Dinculescu and Douglas Eck},
  title         = {Music Transformer},
  year          = {2018},
  archiveprefix = {arXiv},
  eprint        = {1809.04281},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1809.04281},
}

@Misc{liutkus2021relative,
  author        = {Antoine Liutkus and Ondřej Cífka and Shih-Lun Wu and Umut Şimşekli and Yi-Hsuan Yang and Gaël Richard},
  title         = {Relative Positional Encoding for Transformers with Linear Complexity},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2105.08399},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2105.08399},
}

@Comment{jabref-meta: databaseType:bibtex;}
