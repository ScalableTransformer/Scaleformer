\section{\label{sec:introduction}Introduction}\citet{vaswani2017attention} introduced the transformers, a simpler network to model sequence-to-sequence translations tackling new heights of problem complexity. The improvements were due to the non sequential nature of its training process which allowed better parallelization and thus fast training of big models, and due to improved long term dependencies thanks to the intrinsic good gradient flows of the attention mechanism. However the original transformer presented some drawbacks. It has a quadratic complexity with sequences length, what represents a major drawback to its scalability. The positional encoding induce a different initial embedding for the same group of words at different position in the sentence, which is detrimental for generalization. The absolute positional encoding is also detrimental for the extrapolation to sequences longer than training sequences - even for the initially proposed sinusoidal positional encoding.Ever since, significant research effort have been made to alleviate these problems. Several strategies have been proposed to reduce the quadratic complexity with sequence length, and the relative positional encoding further improved the performances of the model. These improvements have opened the path to the application of transformers to image analysis where scalability and positional invariance are of upmost importance. However most proposed architecture we are aware of only alleviated parts of the issues, by being incompatible with relative positional encoding, having no scheme for masked attention - thus being restrained to encoder-only models, or being complex to use/implement - needing custom operations programmed in CUDA or introducing stochastic methods.In the present work, by assembling together and building over ideas developed in several works, we propose a complete encoder-decoder transformer model (with unidirectional and bidirectional attention), with linear complexity with regards to sequence length, compatible with relative positional encoding, and that can be implemented with usual functions of neural network frameworks without requiring custom CUDA code.In the present work we propose a complete encoder-decoder transformer model that scales linearly with large sequence lengths, by putting together ideas developped across several works \cite{vaswani2017attention,shen2020efficient,katharopoulos2020transformers,choromanski2021rethinking,shaw2018selfattention,horn2021translational}. It remains compatible with relative positional encoding, and can be implemented with usual functions of neural network frameworks without requiring custom CUDA code.\endinput